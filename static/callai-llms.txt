# CallAI Helper Function

The `callAI` helper function provides an easy way to make AI requests to OpenAI-compatible model providers.

## Installation

```bash
npm install call-ai
# or
pnpm add call-ai
# or
yarn add call-ai
```

## API Key

You can set the API key in the `window` object:

```javascript
window.CALLAI_API_KEY = "your-api-key";
```

Or pass it directly to the `callAI` function:

```javascript
const response = await callAI("Write a haiku", { apiKey: "your-api-key" });
```

## Basic Usage

By default the function returns a Promise that resolves to the complete response:

```javascript
import { callAI } from 'call-ai';

// Default behavior - returns a Promise<string>
const response = await callAI("Write a haiku");

// Use the complete response directly
console.log(response); // Complete response text
```

## Streaming Mode

If you prefer to receive the response incrementally as it's generated, set `stream: true`. This returns an AsyncGenerator:

```javascript
import { callAI } from 'call-ai';

// Enable streaming mode explicitly - returns an AsyncGenerator
const generator = callAI("Write an epic poem", { stream: true });
// Process the streaming response
for await (const partialResponse of generator) {
  console.log(partialResponse); // Updates incrementally
}
```

## JSON Schema Responses

To get structured JSON responses, provide a schema in the options:

```javascript
import { callAI } from 'call-ai';

// Define your schema
const todoSchema = {
  properties: {
    todos: {
      type: "array",
      items: { type: "string" }
    }
  }
};

// Using with non-streaming (default)
const todoResponse = await callAI("Give me a todo list for learning React", {
  schema: todoSchema
});
const todoData = JSON.parse(todoResponse);
console.log(todoData.todos); // Array of todo items
```

## JSON with Streaming

In this example, we're using the `callAI` helper function to get weather data in a structured format with streaming preview:

```javascript
import { callAI } from 'call-ai';

const weatherSchema = {
  properties: {
    location: {
      type: "string",
      description: "City or location name"
    },
    temperature: {
      type: "number",
      description: "Temperature in Celsius"
    },
    conditions: {
      type: "string",
      description: "Weather conditions description"
    }
  }
};

// Get weather data with streaming updates
const generator = callAI("What's the weather like in Paris today?", {
  schema: weatherSchema,
  stream: true
});

// Display streaming updates as they arrive
const resultElement = document.getElementById('result');
let finalResponse;

for await (const partialResponse of generator) {
  resultElement.textContent = partialResponse;
  finalResponse = partialResponse;
}

// Process final result
try {
  const weatherData = JSON.parse(finalResponse);
  
  // Access individual fields
  const { location, temperature, conditions } = weatherData;
  
  // Update UI with formatted data
  document.getElementById('location').textContent = location;
  document.getElementById('temperature').textContent = `${temperature}Â°C`;
  document.getElementById('conditions').textContent = conditions;
} catch (error) {
  console.error("Failed to parse response:", error);
}
```

## Specifying a Model

By default, the function uses `openrouter/auto` (automatic model selection). You can specify a different model:

```javascript
import { callAI } from 'call-ai';

// Use a specific model via options
const response = await callAI(
  "Explain quantum computing in simple terms", 
  { model: "openai/gpt-4o" }
);

console.log(response);
```

## Additional Options

You can pass extra parameters to customize the request:

```javascript
import { callAI } from 'call-ai';

const response = await callAI(
  "Write a creative story",
  {
    model: "anthropic/claude-3-opus",
    temperature: 0.8,     // Higher for more creativity (0-1)
    max_tokens: 1000,     // Limit response length
    top_p: 0.95           // Control randomness
  }
);

console.log(response);
```

## Message History

For multi-turn conversations, you can pass an array of messages:

```javascript
import { callAI } from 'call-ai';

// Create a conversation
const messages = [
  { role: "system", content: "You are a helpful coding assistant." },
  { role: "user", content: "How do I use React hooks?" },
  { role: "assistant", content: "React hooks are functions that let you use state and other React features in functional components..." },
  { role: "user", content: "Can you show me an example of useState?" }
];

// Pass the entire conversation history
const response = await callAI(messages);
console.log(response);

// To continue the conversation, add the new response and send again
messages.push({ role: "assistant", content: response });
messages.push({ role: "user", content: "What about useEffect?" });

// Call again with updated history
const nextResponse = await callAI(messages);
console.log(nextResponse);
```

## Using with OpenAI API

You can use callAI with OpenAI's API directly by providing the appropriate endpoint and API key:

```javascript
import { callAI } from 'call-ai';

// Use with OpenAI's API
const response = await callAI(
  "Explain the theory of relativity", 
  {
    model: "gpt-4",
    apiKey: "sk-...", // Your OpenAI API key
    endpoint: "https://api.openai.com/v1/chat/completions"
  }
);

console.log(response);

// Or with streaming
const generator = callAI(
  "Explain the theory of relativity", 
  {
    model: "gpt-4",
    apiKey: "sk-...", // Your OpenAI API key
    endpoint: "https://api.openai.com/v1/chat/completions",
    stream: true
  }
);

for await (const chunk of generator) {
  console.log(chunk);
}
```

## Custom Endpoints

You can specify a custom endpoint for any OpenAI-compatible API:

```javascript
import { callAI } from 'call-ai';

// Use with any OpenAI-compatible API
const response = await callAI(
  "Generate ideas for a mobile app",
  {
    model: "your-model-name",
    apiKey: "your-api-key",
    endpoint: "https://your-custom-endpoint.com/v1/chat/completions"
  }
);

console.log(response);
```

## Recommended Models

| Model | Best For | Speed vs Quality |
|-------|----------|------------------|
| `openrouter/auto` | Default, automatically selects | Adaptive |
| `anthropic/claude-3-haiku` | Cost-effective | Fast, good quality |
| `openai/gpt-4o` | Best overall quality | Medium speed, highest quality |
| `anthropic/claude-3-opus` | Complex reasoning | Slower, highest quality |
| `mistralai/mistral-large` | Open weights alternative | Good balance |
