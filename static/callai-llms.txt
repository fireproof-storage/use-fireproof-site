# CallAI Helper Function

The `callAI` helper function provides an easy way to make AI requests to OpenAI-compatible model providers.

## Installation

```bash
npm install call-ai
```

## API Key

You can set the API key in the `window` object:

```javascript
window.CALLAI_API_KEY = "your-api-key";
```

Or pass it directly to the `callAI` function:

```javascript
const response = await callAI("Write a haiku", { apiKey: "your-api-key" });
```

## Basic Usage

By default the function returns a Promise that resolves to the complete response:

```javascript
import { callAI } from 'call-ai';

// Default behavior - returns a Promise<string>
const response = await callAI("Write a haiku");

// Use the complete response directly
console.log(response); // Complete response text
```

## Streaming Mode

If you prefer to receive the response incrementally as it's generated, set `stream: true`. This returns an AsyncGenerator:

```javascript
import { callAI } from 'call-ai';

// Enable streaming mode explicitly - returns an AsyncGenerator
const generator = callAI("Write an epic poem", { stream: true });
// Process the streaming response
for await (const partialResponse of generator) {
  console.log(partialResponse); // Updates incrementally
}
```

## JSON Schema Responses

To get structured JSON responses, provide a schema in the options:

```javascript
import { callAI } from 'call-ai';

const todoResponse = await callAI("Give me a todo list for learning React", {
  schema: {
    name: "todo",  // Optional - defaults to "result" if not provided
    properties: {
      todos: {
        type: "array",
        items: { type: "string" }
      }
    }
  }
});
const todoData = JSON.parse(todoResponse);
console.log(todoData.todos); // Array of todo items
```

## JSON with Streaming

In this example, we're using the `callAI` helper function to get weather data in a structured format with streaming preview:

```javascript
import { callAI } from 'call-ai';

// Get weather data with streaming updates
const generator = callAI("What's the weather like in Paris today?", {
  stream: true,
  schema: {
    properties: {
      location: {
        type: "string",
        description: "City or location name"
      },
      temperature: {
        type: "number",
        description: "Temperature in Celsius"
      },
      conditions: {
        type: "string",
        description: "Weather conditions description"
      }
    }
  }
});

// Preview streaming updates as they arrive, don't parse until the end
const resultElement = document.getElementById('result');
let finalResponse;

for await (const partialResponse of generator) {
  resultElement.textContent = partialResponse;
  finalResponse = partialResponse;
}

// Parse final result
try {
  const weatherData = JSON.parse(finalResponse);
  
  // Access individual fields
  const { location, temperature, conditions } = weatherData;
  
  // Update UI with formatted data
  document.getElementById('location').textContent = location;
  document.getElementById('temperature').textContent = `${temperature}Â°C`;
  document.getElementById('conditions').textContent = conditions;
} catch (error) {
  console.error("Failed to parse response:", error);
}
```

### Schema Structure Recommendations

1. **Flat schemas perform better across all models**. If you need maximum compatibility, avoid deeply nested structures.

2. **Field names matter**. Some models have preferences for certain property naming patterns:
   - Use simple, common naming patterns like `name`, `type`, `items`, `price` 
   - Avoid deeply nested object hierarchies (more than 2 levels deep)
   - Keep array items simple (strings or flat objects)

3. **Model-specific considerations**:
   - **OpenAI models**: Best overall schema adherence and handle complex nesting well
   - **Claude models**: Great for simple schemas, occasional JSON formatting issues with complex structures
   - **Gemini models**: Good general performance, handles array properties well
   - **Llama/Mistral/Deepseek**: Strong with flat schemas, but often ignore nesting structure and provide their own organization

4. **For mission-critical applications** requiring schema adherence, use OpenAI models or implement fallback mechanisms.

### Models for Structured Outputs

- OpenAI models: Best overall schema adherence and handle complex nesting well
- Claude models: Great for simple schemas, occasional JSON formatting issues with complex structures
- Gemini models: Good general performance, handles array properties well
- Llama/Mistral/Deepseek: Strong with flat schemas, but often ignore nesting structure and provide their own organization


## Specifying a Model

By default, the function uses `openrouter/auto` (automatic model selection). You can specify a different model:

```javascript
import { callAI } from 'call-ai';

// Use a specific model via options
const response = await callAI(
  "Explain quantum computing in simple terms", 
  { model: "openai/gpt-4o" }
);

console.log(response);
```

## Additional Options

You can pass extra parameters to customize the request:

```javascript
import { callAI } from 'call-ai';

const response = await callAI(
  "Write a creative story",
  {
    model: "anthropic/claude-3-opus",
    temperature: 0.8,     // Higher for more creativity (0-1)
    max_tokens: 1000,     // Limit response length
    top_p: 0.95           // Control randomness
  }
);

console.log(response);
```

## Message History

For multi-turn conversations, you can pass an array of messages:

```javascript
import { callAI } from 'call-ai';

// Create a conversation
const messages = [
  { role: "system", content: "You are a helpful coding assistant." },
  { role: "user", content: "How do I use React hooks?" },
  { role: "assistant", content: "React hooks are functions that let you use state and other React features in functional components..." },
  { role: "user", content: "Can you show me an example of useState?" }
];

// Pass the entire conversation history
const response = await callAI(messages);
console.log(response);

// To continue the conversation, add the new response and send again
messages.push({ role: "assistant", content: response });
messages.push({ role: "user", content: "What about useEffect?" });

// Call again with updated history
const nextResponse = await callAI(messages);
console.log(nextResponse);
```

## Using with OpenAI API

You can use callAI with OpenAI's API directly by providing the appropriate endpoint and API key:

```javascript
import { callAI } from 'call-ai';

// Use with OpenAI's API
const response = await callAI(
  "Explain the theory of relativity", 
  {
    model: "gpt-4",
    apiKey: "sk-...", // Your OpenAI API key
    endpoint: "https://api.openai.com/v1/chat/completions"
  }
);

console.log(response);

// Or with streaming
const generator = callAI(
  "Explain the theory of relativity", 
  {
    model: "gpt-4",
    apiKey: "sk-...", // Your OpenAI API key
    endpoint: "https://api.openai.com/v1/chat/completions",
    stream: true
  }
);

for await (const chunk of generator) {
  console.log(chunk);
}
```

## Custom Endpoints

You can specify a custom endpoint for any OpenAI-compatible API:

```javascript
import { callAI } from 'call-ai';

// Use with any OpenAI-compatible API
const response = await callAI(
  "Generate ideas for a mobile app",
  {
    model: "your-model-name",
    apiKey: "your-api-key",
    endpoint: "https://your-custom-endpoint.com/v1/chat/completions"
  }
);

console.log(response);
```

## Recommended Models

| Model | Best For | Speed vs Quality |
|-------|----------|------------------|
| `openrouter/auto` | Default, automatically selects | Adaptive |
| `anthropic/claude-3-haiku` | Cost-effective | Fast, good quality |
| `openai/gpt-4o` | Best overall quality | Medium speed, highest quality |
| `anthropic/claude-3-opus` | Complex reasoning | Slower, highest quality |
| `mistralai/mistral-large` | Open weights alternative | Good balance |

## Aliens Example

```javascript
import { callAI } from 'call-ai';

// Making the call with message array and schema
const generator = callAI([
  {
    role: "user",
    content: "Generate 3 unique alien species with unique biological traits, appearance, and preferred environments. Make them scientifically plausible but creative."
  }
], {
  stream: true,
  schema: {
    properties: {
      aliens: {
        type: "array",
        items: {
          type: "object",
          properties: {
            name: {
              type: "string"
            },
            description: {
              type: "string"
            },
            traits: {
              type: "array",
              items: {
                type: "string"
              }
            },
            environment: {
              type: "string"
            }
          }
        }
      }
    }
  }
});

// Process the streaming response
for await (const partialResponse of generator) {
  console.log(partialResponse); // Will show the JSON being built incrementally
}

// After streaming is complete, you can parse the final response
const alienData = JSON.parse(/*final response*/);
console.log(alienData.aliens); // Array of alien species
```

## Cyberpunk Fortune Example

```javascript
const demoData = await callAI("Generate 4 fictional cyberpunk fortune scenarios with name, desire, fear, mood (from: elated, hopeful, neutral, anxious, defeated), and fortune text. Return as structured JSON with these fields.", {
  schema: {
    properties: {
      fortunes: {
        type: "array",
        items: {
          type: "object",
          properties: {
            name: { type: "string" },
            desire: { type: "string" },
            fear: { type: "string" },
            mood: { type: "string" },
            fortune: { type: "string" }
          }
        }
      }
    }
  }
});
```

## Error Handling

The library provides consistent error handling for both streaming and non-streaming modes:

```javascript
import { callAI } from 'call-ai';

try {
  const response = await callAI("Generate some content", {
    apiKey: "invalid-key" // Invalid or missing API key
  });
  
  // If there was an error, response will be a JSON string with error details
  try {
    const errorObj = JSON.parse(response);
    if (errorObj.message && errorObj.error) {
      console.error("API error:", errorObj.message);
    } else {
      // Process normal response
      console.log(response);
    }
  } catch {
    // Not an error JSON, process normal response
    console.log(response);
  }
} catch (e) {
  // Handle any unexpected errors
  console.error("Unexpected error:", e);
}
```

For streaming, error handling works similarly:

```javascript
import { callAI } from 'call-ai';

try {
  const generator = callAI("Generate some content", {
    apiKey: "invalid-key", // Invalid or missing API key
    stream: true
  });
  
  // Consume the generator
  let finalResponse = '';
  for await (const chunk of generator) {
    finalResponse = chunk;
  }
  
  // Check if the final response is an error
  try {
    const errorObj = JSON.parse(finalResponse);
    if (errorObj.message && errorObj.error) {
      console.error("API error:", errorObj.message);
    } else {
      // Process final response
      console.log(finalResponse);
    }
  } catch {
    // Not an error JSON, process normal response
    console.log(finalResponse);
  }
} catch (e) {
  // Handle any unexpected errors
  console.error("Unexpected error:", e);
}
```
